# Backpropagation y Gradient Descent

Este proceso está relacionado con el **aprendizaje supervisado**, que como hemos visto antes, es el hecho de tomar el error entre una salida esperada y la salida obtenida por la red neuronal, de esta manera se pueden ajustar los pesos y bias entre las capas para acercarse de la mejor manera a la respuesta correcta

## Costo de un sistema

Cuando hablamos de _costo_ primero debemos recordar que es el error; que resulta ser simplemente la diferencia entre las salidas obtenida y esperadas. En una red neuronal, expandimos esta definición más allá; _costo_ es el error acumulado para un conjunto grande de datos (usados para entrenar una red neuronal)

## Conceptos básicos

Backpropagation es un algoritmo que altera los pesos de una red neuronal, propagando el error de la salida del sistema hacia atrás, de ahí el nombre.

Debes tener en cuenta que esta es una implementación bastante simple del algoritmo, dicho esto, la meta principal es tener un entendimiento básico de cómo funciona, de esta manera tendrás una mejor oportunidad de entender herramientas de machine-learning más robustas.

## Como funciona

Miremos el esquema de una red neuronal bastante simple, donde están solo las neuronas de la capa escondida y de salida:

![esquema_nn_simple](/docs/img/simple_nn_backpropagation1.jpg)

La principal diferencia entre la formula con _&Delta;W_ que calculábamos previamente en [aprendizaje supervisado](docs/spa/1.perceptron/2.aprendizaje_supervisado.md) es que para **Backpropagation** lo que realmente necesitamos hacer, es determinar que peso es el que tiene más influencia en el error, ya que ahora debemos calcular los errores independientes para cada neurona escondida.

Si es que observamos un diagrama completo, podemos ver realmente a que errores nos referíamos anteriormente.

![diagrama_completo](/docs/img/complete_nn_backpropagation1.jpg)

Como vemos, tenemos que calcular _err<sub>o<sub>1</sub></sub>, err<sub>h<sub>1</sub></sub>_ y _err<sub>h<sub>2</sub></sub>_

Estos dos últimos errores _err<sub>h<sub>1</sub></sub>_ y _err<sub>h<sub>2</sub></sub>_ serán usados para alterar los pesos entre la capa escondida y, de entrada, con el algoritmo de _descenso en gradiente_

Así que ahora podemos calcular los errores de la siguiente manera:

El error en la capa de salida es el mismo que habíamos revisado antes, _err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

Ahora podemos calcular _err<sub>h<sub>1</sub></sub>_ y _err<sub>h<sub>2</sub></sub>_ como porciones de _err<sub>o<sub>1</sub></sub>_, así que ahora tenemos:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>1</sub></sup> &frasl; <sub>( w<sub>1</sub>+w<sub>2</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

Y

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>2</sub></sup> &frasl; <sub>( w<sub>2</sub>+w<sub>1</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

Aun así, este caso es uno particular, con una sola neurona de salida, ¿qué es lo que sucede cuando tenemos múltiples neuronas de salida o múltiples capas escondidas?

![esquema_completo_2](/docs/img/complete_nn_backpropagation2.jpg)

Ahora tenemos:

_err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

_err<sub>o<sub>2</sub></sub> = y<sub>known<sub>2</sub></sub> - y<sub>guess<sub>2</sub></sub>_

Con esta definición, podemos calcular el error de la capa de salida para un numero arbitrario de neuronas de salida, solo debemos seguir incrementando el número de errores individuales (_err<sub>o<sub>n</sub></sub> = y<sub>known<sub>n</sub></sub> - y<sub>guess<sub>n</sub></sub>_).

Por otra parte, para los errores de la capa oculta, ahora tenemos que tomar en cuenta la mayor cantidad de conexiones que existen al tener más neuronas en la capa de salida.

Lo hacemos de esta manera:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>11</sub></sup> &frasl; <sub>( w<sub>11</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>21</sub></sup> &frasl; <sub>( w<sub>21</sub>+w<sub>22</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

Y para _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>12</sub></sup> &frasl; <sub>( w<sub>12</sub>+w<sub>11</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>22</sub></sup> &frasl; <sub>( w<sub>22</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

Podemos generalizar esta fórmula para cualquier número de neuronas escondidas y de salida, de la siguiente manera:

![formula general](/docs/img/general_hidden_err_formula.jpg)

Con _n_ siendo la cantidad de neuronas escondidas y _w_ la variable que representa a los pesos de las conexiones.

## Implementando el concepto en una Red Neuronal

El objetivo principal de este algoritmo es calcular los errores entre las capas, para luego ajustar los pesos del sistema; una de las maneras más sencillas para calcularlos es la siguiente:

Como ya establecimos, tenemos las siguientes fórmulas para los errores:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>11</sub></sup> &frasl; <sub>( w<sub>11</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>21</sub></sup> &frasl; <sub>( w<sub>21</sub>+w<sub>22</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

Y para _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>12</sub></sup> &frasl; <sub>( w<sub>12</sub>+w<sub>11</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>22</sub></sup> &frasl; <sub>( w<sub>22</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

Si es que ignoramos el denominador que normaliza los términos ahora tenemos:

_err<sub>h<sub>1</sub></sub> = w<sub>11</sub> * err<sub>o<sub>1</sub></sub> + w<sub>21</sub>* err<sub>o<sub>2</sub></sub>_

Y para _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = w<sub>12</sub>* err<sub>o<sub>1</sub></sub> + w<sub>22</sub>* err<sub>o<sub>2</sub></sub>_

Si miramos atentamente a estas nuevas fórmulas notaremos que se parecen a un producto entre matrices, de hecho, si es que tenemos nuestra matriz de pesos _H (refiriéndonos a los pesos entre la capa oculta y de salida)_ definida como:

![matriz de pesos](/docs/img/weight_matrix_hidden_output.jpg)

Y nuestra matriz de errores de salida _E_ de esta manera:

![matriz de errores](/docs/img/output_error_matrix.jpg)

Podríamos pensar que la matriz de error resultante es el producto entre estas matrices.

![Matriz equivocada](/docs/img/result_error_matrix_wrong.jpg)

Si es que observamos los índices de esta matriz y los comparamos con los que teníamos en las formulas anteriores, sabremos que nuestra operación no es la correcta, de hecho, para corregir esto, necesitamos usar la transpuesta de la matriz de pesos, así que ahora será: _H<sup>T</sup>* E_.

La matriz transpuesta sera:

![transposed weight matrix](/docs/img/weight_matrix_hidden_output_transposed.jpg)

Y la operación correcta daría como resultado:

![matriz de resultado correcta](/docs/img/result_error_matrix.jpg)

Así que ahora tenemos:

! [matriz completa de resultados](/docs/img/complete_result_error_matrix.jpg)

Además, este proceso puede ser definido incluso para una red neuronal de múltiples capas ocultas; solo se debe continuar con la misma operación hacia las capas anteriores.

## Gradient Descent

Como habíamos revisado anteriormente, este algoritmo es un proceso matemático que busca el mínimo gradiente de una función basado en el error de una salida concreta respecto a la respuesta correcta obtenida de la salida de esta función.

Implementamos una versión bastante simplificada del algoritmo, con la función _y = mx + b_; para la cual queríamos encontrar los cambios _&Delta;m_ y _&Delta;b_ que ajustarían la función hacia nuestra salida deseada. Para calcular estos valores teníamos las siguientes formulas:

&Delta;m = _Lr \* x \* err_

&Delta;b = _Lr * err_

Ahora tenemos que traducirlas a un espacio multi-dimensional, así que, en vez de tener una función enteramente lineal, ahora tenemos: Y = &sigma;( W*I+B )_ (_&sigma;(X)_ la cual es nuestra función de activación) con _W_ e _I_ como las matrices de pesos y valores de entrada y _B_ como la matriz columna para el bias

En vez de calcular _&Delta;m_ ahora queremos calcular el cambio para los pesos _&Delta;W_

Así que _&Delta;W<sub>ih</sub> = Lr * gradient * I<sup>T</sup>_

Nota: _Lr_ es un escalar.

¿Pero que es _gradient_?, es el gradiente de la función que tenemos como entrada. Si revisamos a nuestra función lineal, _gradient_ solo estaba definido como el error por _Lr_, ahora para _&sigma;(X)_ el está definida como _Lr_ por el error y esto a su vez multiplicado por  _&sigma;**'** (X)_ que está definida como _&sigma;(X)*( 1 - &sigma;(X))_

Una breve demostración para ello:

![demostración](/docs/img/sigma_derivative_demostration.jpg)

Así que nuestra formula puede ser representada como: _&Delta;W = Lr * E<sub>layer</sub> * &sigma;'( O<sub>layer</sub> ) x I<sup>T</sup>_ con _E_ como la matriz columna de errores, _O_ la salida de la capa y _H<sup>T</sup>_ como la matriz transpuesta de entradas para esa respectiva capa.

Nota: _*_ se refiere a la multiplicación por elementos entre matrices, o el producto de Hadamard, _x_ al producto original de matrices y _&sigma;'(X)_ es una operación aplicada a cada elemento.

Para los pesos de cada bias, como hemos visto antes, usaremos la misma fórmula, solo que no tenemos la necesidad de usar la matriz de entradas: _&Delta;W<sub>b</sub> = Lr * E<sub>layer</sub> * &sigma;'( O<sub>layer</sub> )_.