# Backpropagation and Gradient Descent

This process ties to **Supervised learning**, which as we have discussed it before, is the process in which the neural network learns by looking at the error between the desired and computed outputs, so it can adjust its weights and biases to better match the correct answer.

## Defining cost

To talk about the _cost_ of a system, we need to have a clear definition of error, which is the difference between the expected and produced outputs. In a neural network, we expand this definition further; _cost_ is the cumulative error for a large data set (used for training the neural network).

## Basics

Backpropagation is an algorithm that changes the weights of a neural network, propagating the error from the output layer backwards, hence the name.

Have in mind that this is a simplistic implementation of this algorithm; the main goal is to learn how it works, for you to have a better understanding of more powerful machine-learning tools.

## How it works

Let us look at a simple neural network scheme, where we only look at the hidden and output layers:

![simple_nn_scheme](/docs/img/simple_nn_backpropagation1.jpg)

The key difference from the _&Delta;W_ formula that we calculated before in [supervised learning](/docs/eng/1.perceptron/2.supervised_learning.md) is that for **Backpropagation** we really need to define which weight is responsible for the _error_, as we need to calculate independent errors for all the hidden layer neurons.

If we go to a full diagram, we can really see where we need to calculate these errors.

![complete_nn_scheme](/docs/img/complete_nn_backpropagation1.jpg)

As we see, we need to calculate _err<sub>o<sub>1</sub></sub>, err<sub>h<sub>1</sub></sub>_ and _err<sub>h<sub>2</sub></sub>_

This last two errors _err<sub>h<sub>1</sub></sub>_ and _err<sub>h<sub>2</sub></sub>_ will be used to tweak the weights between the input and hidden layers, with the _gradient descent_ algorithm.

Therefore, this is how the errors are computed:

The error at the output layer is the same as before,  _err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

Now we can calculate _err<sub>h<sub>1</sub></sub>_ and _err<sub>h<sub>2</sub></sub>_ as a portion of _err<sub>o<sub>1</sub></sub>_, now we have:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>1</sub></sup> &frasl; <sub>( w<sub>1</sub>+w<sub>2</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

And

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>2</sub></sup> &frasl; <sub>( w<sub>2</sub>+w<sub>1</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

However, this is a particular case, with one output neuron, what happens if we have multiple output neurons or multiple hidden layers?

![complete_nn_scheme_2](/docs/img/complete_nn_backpropagation2.jpg)

Now we have:

_err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

_err<sub>o<sub>2</sub></sub> = y<sub>known<sub>2</sub></sub> - y<sub>guess<sub>2</sub></sub>_

Moreover, this holds for any number of output neurons we have, we just keep incrementing the number of individual errors (_err<sub>o<sub>n</sub></sub> = y<sub>known<sub>n</sub></sub> - y<sub>guess<sub>n</sub></sub>_).

Nevertheless, for the hidden layer errors, now we have to take into consideration the larger amount of connections.

Like this:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>11</sub></sup> &frasl; <sub>( w<sub>11</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>21</sub></sup> &frasl; <sub>( w<sub>21</sub>+w<sub>22</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

And for _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>12</sub></sup> &frasl; <sub>( w<sub>12</sub>+w<sub>11</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>22</sub></sup> &frasl; <sub>( w<sub>22</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

We could generalize this formula, for any number of hidden neurons and output neurons like this:

![scheme formula](/docs/img/general_hidden_err_formula.jpg)

With _n_ being the hidden neuron index and _w_ being the weights variable.

## Implementing the concept on a Neural Network

The main goal of this algorithm is to calculate the errors between layers, to later adjust the weights of the system; the easiest way to implement the calculation is as follows:

As we stablished, we have the following formulas for the hidden layer errors:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>11</sub></sup> &frasl; <sub>( w<sub>11</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>21</sub></sup> &frasl; <sub>( w<sub>21</sub>+w<sub>22</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

And for _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>12</sub></sup> &frasl; <sub>( w<sub>12</sub>+w<sub>11</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>22</sub></sup> &frasl; <sub>( w<sub>22</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

If we ignore the normalizing bit on the denominator of each term, then we have:

_err<sub>h<sub>1</sub></sub> = w<sub>11</sub> * err<sub>o<sub>1</sub></sub> + w<sub>21</sub>* err<sub>o<sub>2</sub></sub>_

And for _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = w<sub>12</sub>* err<sub>o<sub>1</sub></sub> + w<sub>22</sub>* err<sub>o<sub>2</sub></sub>_

If we look closely this looks a lot like a matrix product. In fact, we had our weight matrix _H (for the weight matrix between the hidden and output layers)_ defined like this:

![weight matrix](/docs/img/weight_matrix_hidden_output.jpg)

And our output error matrix _E_ like this:

![error matrix](/docs/img/output_error_matrix.jpg)

You could think that the result error matrix is the product between the hidden weights matrix _H_ and the output error matrix _E_.

![wrong result matrix](/docs/img/result_error_matrix_wrong.jpg)

But if you watched the indexes of the previous formulas closely, you may have noticed that the operation we just made is incorrect, in fact to match the formulas we have defined, we need to use the weight matrix **transposed**, so now it would be _H<sup>T</sup>* E_.

The transposed weight matrix would be:

![transposed weight matrix](/docs/img/weight_matrix_hidden_output_transposed.jpg)

The correct operation would be:

![correct result matrix](/docs/img/result_error_matrix.jpg)

So now, we have:

![complete result matrix](/docs/img/complete_result_error_matrix.jpg)

Moreover, this process would work for a multiple hidden layer network; you just keep doing the product between the error matrix and the weight matrix transposed.

## Gradient Descent

As before, this algorithm is a mathematical process that seeks for the minimum gradient of a function based on the error of a given output respect to the correct output from the function.

We implemented a simple version of the algorithm with the function _y = mx + b_, (bear in mind that we used the weights to resemble the line function) and now we want to implement the same concept but in a multi-dimensional space.

With a line function _y = mx + b_; we want to find the changes _&Delta;m_ and _&Delta;b_ which will adjust the function into our desired output. The way we calculate those variables is:

&Delta;m = _Lr \* x \* err_

&Delta;b = _Lr * err_

Now we have to translate this formulas to our multi-dimensional space, so instead of the line function we have that _Y = &sigma;( W*I+B )_ (_&sigma;(X)_ was our activation function) With _W_ and _I_ as the weight and input matrices and _B_ for the bias column matrix.

Instead of _&Delta;m_ we now want the change in weights _&Delta;W_

So _&Delta;W<sub>ho</sub> = Lr * gradient * H<sup>T</sup>_

And _&Delta;W<sub>ih</sub> = Lr * gradient * I<sup>T</sup>_

Note: _Lr_ is a scalar.

But what is the _gradient_ that we are looking for?, previously if we look at our line function, it was the only the error, times learning rate, now for _&sigma;(X)_ is the learning rate, times the error, times _&sigma;**'** (X)_ which is defined as _&sigma;(X)*( 1 - &sigma;(X))_

Demonstration:

![demonstration](/docs/img/sigma_derivative_demostration.jpg)

So our formula for the weight deltas now is: _&Delta;W = Lr * E<sub>layer</sub> * &sigma;'( O<sub>layer</sub> ) x I<sup>T</sup>_ with _E_ being the output error column matrix from the layer, _O_, the output form the layer, _H<sup>T</sup>_, the inputs for the layer transposed.

Note: _*_ refers to element-wise multiplication or the Hadamard product between matrices, _x_ to the original matrix product, and _&sigma;'(X)_ is an element-wise applied function.

In addition, for the bias weights as we have seen before, is the same formula but without the input matrix of the layer transposed:  _&Delta;W<sub>b</sub> = Lr * E<sub>layer</sub> * &sigma;'( O<sub>layer</sub> )_.